# Vision-Based Hand Gesture Recognition for Smart Home Control ğŸ âœ‹

**Senior Design Project - CMPE 491** **Istanbul Bilgi University**

**Authors:** Talha Erden, Emre SaygÄ±n  
**Advisor:** Prof. Dr. DoÄŸan Ã‡Ã¶rÃ¼ÅŸ

## ğŸ“– Overview
This project bridges the gap between smart home technology and accessibility for hearing-impaired individuals. Instead of relying on voice assistants (like Alexa or Siri), this system allows users to control home appliances (lights, fans) using **static hand gestures** captured by a standard webcam.

The system achieves **99.8% accuracy** using a lightweight Artificial Neural Network (ANN) trained on skeletal features extracted by **Google MediaPipe**, making it efficient enough to run on standard CPUs without a GPU.

![System Architecture](assets/system_architecture.png)

## ğŸ“Š Performance Analysis

The model demonstrates robust learning capabilities with minimal overfitting, achieving **99.8% validation accuracy** after 50 epochs.

### 1. Training Metrics
The graphs below visualize the Loss and Accuracy progression. The convergence of training and validation lines indicates stable learning.

![Training Graphs](assets/training_graphs.png)

### 2. Confusion Matrix
The model classifies 4 distinct gestures with high precision. Misclassification is negligible.

* **LIGHT_ON:** Open Palm ğŸ–ï¸
* **LIGHT_OFF:** Closed Fist âœŠ
* **FAN_ON:** 'V' Sign âœŒï¸
* **FAN_OFF:** Index Pointing â˜ï¸

![Confusion Matrix](assets/confusion_matrix.png)

## ğŸ¥ Real-Time Demonstration
The system captures hand landmarks in real-time using MediaPipe and classifies them instantly.

![Real Time Demo](assets/mediapipe_image.png) 

## ğŸš€ Key Features
* **Contactless Control:** Hygienic and accessible interaction.
* **High Accuracy:** 99.8% validation accuracy on a custom dataset of 9,000 images.
* **Low Latency:** Uses skeletal tracking (21 landmarks) instead of heavy CNN image processing.
* **Real-Time Actuation:** Integrates with Arduino via Serial Communication to control physical relays/LEDs.

## ğŸ› ï¸ Tech Stack
* **Language:** Python 3.x
* **Computer Vision:** OpenCV, Google MediaPipe
* **Deep Learning:** TensorFlow / Keras (ANN)
* **Hardware:** Arduino Uno R3, Relay Modules
* **Communication:** PySerial (USB Serial)

## ğŸ“‚ Project Structure

The project is organized as follows:

```text
â”œâ”€â”€ arduino/           # C++ code (.ino) for Arduino Uno
â”œâ”€â”€ dataset/           # Processed landmark dataset (CSV format)
â”œâ”€â”€ models/            # Pre-trained .h5 models and label encoders
â”œâ”€â”€ python/            # Python source codes (Training & Inference)
â”‚   â”œâ”€â”€ collect_landmarks.py   # Script to extract landmarks from raw images
â”‚   â”œâ”€â”€ train_model.py         # Script to train the ANN model
â”‚   â””â”€â”€ inference_serial.py    # Main script for Real-Time Control
â”œâ”€â”€ requirements.txt   # List of dependencies
â””â”€â”€ README.md          # Project documentation
